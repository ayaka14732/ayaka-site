<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="zh-CN" xml:lang="zh-CN" dir="ltr">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="三日月綾香" />
  <meta name="keywords" content="概率图模型, 机器学习, 自然语言处理, 学习感想" />
  <title>概率图模型课程学习感想</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <link rel="stylesheet" href="/pandoc.css" />
  <link rel="stylesheet" href="/sans.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <!-- Others -->
  <meta property="og:image" content="https://avatars2.githubusercontent.com/u/68557794?s=400&u=343da7bebff676129e87341def71121ebffc4c9c&v=4"/>
  <script>
    window.addEventListener('DOMContentLoaded', function() {
      var xs = document.getElementsByTagName('code');
      for (var i = 0, len = xs.length; i < len; i++)
        xs[i].lang = 'en-x-code';
      xs = document.getElementsByClassName('footnote-back');
      for (var i = 0, len = xs.length; i < len; i++)
        xs[i].lang = 'en-x-code';
      document.querySelectorAll('div.sourceCode > pre').forEach(function(x) {
        x.classList.add('numberSource');
      });
    });
  </script>
  
</head>
<body>
<header id="title-block-header">
<h1 class="title">概率图模型课程学习感想</h1>
<p class="author">三日月綾香</p>
</header>
<blockquote>
<p>注：这篇文章可以看作我学习机器学习的一个阶段性的总结。</p>
</blockquote>
<p>学这门课的时候，整个学期都在纠结中度过。一直在问自己，花比其他课程多无数倍的时间，学这门以后可能永远用不上的课程，对我来说有什么意义。因为我的目标是自然语言处理，我知道现在的做法是深度学习，用「预训练模型+在下游任务上微调」的方式解决问题，这门课的内容在自然语言处理领域已经过时了。</p>
<p>但是，正是这门课促使我思考，机器学习领域到底是怎样发展到现在的。学了这门课以后，我想的不再只是「神经网络怎么调参」，而是会开始思考过去的方法是什么样的，为什么被新方法替代，以后又会走向何方。</p>
<p>我最开始接触自然语言处理，是因为觉得语言很有规则，其实这就对应传统的写规则的方法。随着模型的精度越来越高，人们发现总有一些规则覆盖不到的地方，这时深度学习的优势就体现出来了。从写规则到现在的深度学习，其实是一种思维方式的转变：以前是直接告诉机器「语言的规则是什么」，后来发现这不是三言两语能说清的，所以转变成研究「为了让机器学会语言，哪些结构是必要的」，然后让机器自己来学会。</p>
<p>而这门课所讲的概率图模型，属于传统机器学习的方法，从发展历程上处于写规则和深度学习之间。自然语言处理有基于规则和基于统计两类方法。写规则就是基于规则的方法；而传统机器学习是由统计学发展而来，深度学习又是由传统机器学习发展而来，这两者都属于基于统计的方法。从规则到统计，其实就是对「人类语言并不总是规则的」这一现象的一种妥协，所以才会有「我每开除一个语言学家，语音识别系统准确率又提升了」的梗。那么，从传统机器学习到深度学习，背后的原因又是什么呢？</p>
<p>经过思考，我觉得原因在于：传统机器学习仍然是基于假设构建模型，只不过这里的假设由「语法规则」变成了「数学规则」。在基于规则的方法中，语法规则就相当于假设，也就是假定输入是符合语法的，然后程序依照规则进行处理；在传统机器学习方法中，也是假设输入具有某种统计上的性质，然后根据这些性质构建模型，再得出结果。比如在词性标注问题中，会假设当前词性只和前一个词性相关、当前单词只和当前词性相关，这样才能符合齐次马尔可夫假设、观测独立性假设，才能写成隐马尔可夫模型，然后才能应用维特比算法得出结果。但是，假设总会有例外情况，所以就会出现问题。网上有个提问「machine learning 难点在哪」，题主说「感觉这些算法内容说穿了毫无难度，甚至可以说太过简单」。我看到这句话的第一反应是这个人数学很好，但后来想了想并不只是数学的原因。这个人可能觉得公式简单，比如 log 简单；但我试图把方法套在自然语言处理上的时候，就会想「人类语言中哪一个语法对应 log 呢？如果没有，那就是复杂的」。</p>
<p>而深度学习相对于传统机器学习的区别，就在于它从自底向上的「假设+模型」转变为了自顶向下的「直觉+结构」。直觉告诉我们「同样的东西会出现在输入的不同位置」，所以有了卷积神经网络；直觉告诉我们「人类读句子是按顺序一个词一个词读」，所以有了循环神经网络；直觉告诉我们「一个句子里每个单词之间的相关性可以是乱序的」，所以有了注意力机制。也就是说，我们可以利用直觉，由数据本身出发构建模型，根据数据的特征去使用不同的网络结构。而且，模型计算都被统一成了唯一的方法：误差反向传播。</p>
<p>但是「抛弃规则」也存在弊端，就是让深度学习变成不可解释的「黑盒」。在传统的机器学习中，看着公式就可以解释模型的输出，比如可以指着 KL 散度说「这就是为什么 EM 算法一定收敛」；而在深度学习中就没有这样的公式。不过另一方面，如果要解释「深度学习为什么有用」，或者「为什么深度学习这一个方法可以替代传统机器学习那么多的方法」，那就是通用近似定理：神经网络可以拟合任意函数。所以，「机器学习的可解释性」可以从微观和宏观两个角度来理解：微观就是「指着公式，说模型为什么这样输出」，而宏观则是「神经网络的规模增大时，模型的学习能力可以增加多少」。</p>
<p>由此可以看出，从写规则到传统机器学习再到深度学习，本质就是「放手」。写规则就是人类手把手教机器一个句子怎么写；传统机器学习就是退一步，给定统计假设，得到统计模型，让机器根据模型统计出句子怎么写；深度学习就是再退一步，根据自然语言本身的特征构造合适的神经网络，机器自然就有能力学习到句子怎么写；而未来，应该是构造出新的神经网络，让机器自己掌握学习的方法。</p>
<p>另外，统计学根据「模型参数是常量还是也是随机变量」的不同分为频率派和贝叶斯派。在机器学习中，频率派发展为统计机器学习，而贝叶斯派发展为概率图模型。目前的深度学习主要是由频率派发展而来，不过深度学习中也有一些模型是由贝叶斯派发展而来。将贝叶斯方法引入深度学习很有意义，比如在贝叶斯派的视角下，句子的生成可以理解为从目标分布中采样，在某些任务中可以得到更好的结果。</p>
<p>（作于 2021 年 11 月 22 日，发布于 2021 年 11 月 27 日）</p>
</body>
</html>
